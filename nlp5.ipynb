{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp5",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkpcHsV8RWHA"
      },
      "source": [
        "## Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAQBOJRARev7"
      },
      "source": [
        "**Написать теггер на данных с руским языком**\n",
        "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации  \n",
        "2. написать свой теггер как на занятии, но улучшить попробовать разные векторайзеры, добавить знание не только букв и слов но и совместно объединить эти признаки  \n",
        "3. вместо векторайзеров взять эмбединги попробовать (word2vec и fasttext по желанию дополнительно можно взять tf.keras.layers.Embedding)  \n",
        "4. взять не только эмбединги каждого слова, но и взять соседей, т.е. информацию о соседях количество соседей выбрать самим (узнать наилучшее количество соседей)    \n",
        "5. сравнить все реализованные методы сделать выводы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_16J0ER8WOJx"
      },
      "source": [
        "## загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPRx8Cu_RDY1",
        "outputId": "dbb24f44-aef6-4fbf-be1d-1b5df27ff015"
      },
      "source": [
        "!pip install pyconll"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyconll in /usr/local/lib/python3.7/dist-packages (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wgL-33mWUyZ"
      },
      "source": [
        "import pyconll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXxwW9NzW570",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9adb14c-d767-4763-88e3-f8b71c6df0e5"
      },
      "source": [
        "!mkdir datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘datasets’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpwgA3svWiRw",
        "outputId": "c58429d2-89fe-4cba-d170-5cb48c1f385f"
      },
      "source": [
        "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
        "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-11 11:36:17--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81043533 (77M) [text/plain]\n",
            "Saving to: ‘./datasets/ru_syntagrus-ud-train.conllu’\n",
            "\n",
            "./datasets/ru_synta 100%[===================>]  77.29M   170MB/s    in 0.5s    \n",
            "\n",
            "2021-04-11 11:36:18 (170 MB/s) - ‘./datasets/ru_syntagrus-ud-train.conllu’ saved [81043533/81043533]\n",
            "\n",
            "--2021-04-11 11:36:18--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10903424 (10M) [text/plain]\n",
            "Saving to: ‘./datasets/ru_syntagrus-ud-dev.conllu’\n",
            "\n",
            "./datasets/ru_synta 100%[===================>]  10.40M  52.4MB/s    in 0.2s    \n",
            "\n",
            "2021-04-11 11:36:18 (52.4 MB/s) - ‘./datasets/ru_syntagrus-ud-dev.conllu’ saved [10903424/10903424]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oymo30RBWjjl"
      },
      "source": [
        "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
        "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eCfFPQrsfyo"
      },
      "source": [
        "def conver_to_test_data(full):\n",
        "    res = []\n",
        "    for sent in full:\n",
        "        sub_res = []\n",
        "        for token in sent:\n",
        "            sub_res.append((token.form, token.upos))\n",
        "        res.append(sub_res)\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_9FlV0zvsg0"
      },
      "source": [
        "def conver_to_test_sent(full):\n",
        "    res = []\n",
        "    for sent in full:\n",
        "        sub_res = []\n",
        "        for token in sent:\n",
        "            sub_res.append(token.form)\n",
        "        res.append(sub_res)\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBzFe82cXGNK",
        "outputId": "07bc4c5c-ef70-443f-f949-31b77a9dbe7b"
      },
      "source": [
        "for sent in full_train[:1]:\n",
        "    for token in sent:\n",
        "        print(token.form, token.upos)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Анкета NOUN\n",
            ". PUNCT\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii6ljtxdx2oK"
      },
      "source": [
        "train_data = conver_to_test_data(full_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4DNQT7XszMt",
        "outputId": "224a4eb7-1c06-4762-cd7d-1bf1591d0c35"
      },
      "source": [
        "test_data = conver_to_test_data(full_test)\n",
        "print(test_data[0:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[('Алгоритм', 'NOUN'), (',', 'PUNCT'), ('от', 'ADP'), ('имени', 'NOUN'), ('учёного', 'NOUN'), ('аль', 'PART'), ('-', 'PUNCT'), ('Хорезми', 'PROPN'), (',', 'PUNCT'), ('-', 'PUNCT'), ('точный', 'ADJ'), ('набор', 'NOUN'), ('инструкций', 'NOUN'), (',', 'PUNCT'), ('описывающих', 'VERB'), ('порядок', 'NOUN'), ('действий', 'NOUN'), ('исполнителя', 'NOUN'), ('для', 'ADP'), ('достижения', 'NOUN'), ('результата', 'NOUN'), ('решения', 'NOUN'), ('задачи', 'NOUN'), ('за', 'ADP'), ('конечное', 'ADJ'), ('время', 'NOUN'), ('.', 'PUNCT')], [('В', 'ADP'), ('старой', 'ADJ'), ('трактовке', 'NOUN'), ('вместо', 'ADP'), ('слова', 'NOUN'), ('\"', 'PUNCT'), ('порядок', 'NOUN'), ('\"', 'PUNCT'), ('использовалось', 'VERB'), ('слово', 'NOUN'), ('\"', 'PUNCT'), ('последовательность', 'NOUN'), ('\"', 'PUNCT'), (',', 'PUNCT'), ('но', 'CCONJ'), ('по', 'ADP'), ('мере', 'NOUN'), ('развития', 'NOUN'), ('параллельности', 'NOUN'), ('в', 'ADP'), ('работе', 'NOUN'), ('компьютеров', 'NOUN'), ('слово', 'NOUN'), ('\"', 'PUNCT'), ('последовательность', 'NOUN'), ('\"', 'PUNCT'), ('стали', 'VERB'), ('заменять', 'VERB'), ('более', 'ADV'), ('общим', 'ADJ'), ('словом', 'NOUN'), ('\"', 'PUNCT'), ('порядок', 'NOUN'), ('\"', 'PUNCT'), ('.', 'PUNCT')], [('Это', 'PRON'), ('связано', 'VERB'), ('с', 'ADP'), ('тем', 'PRON'), (',', 'PUNCT'), ('что', 'SCONJ'), ('работа', 'NOUN'), ('каких-то', 'DET'), ('инструкций', 'NOUN'), ('алгоритма', 'NOUN'), ('может', 'VERB'), ('быть', 'AUX'), ('зависима', 'ADJ'), ('от', 'ADP'), ('других', 'ADJ'), ('инструкций', 'NOUN'), ('или', 'CCONJ'), ('результатов', 'NOUN'), ('их', 'DET'), ('работы', 'NOUN'), ('.', 'PUNCT')], [('Таким', 'DET'), ('образом', 'NOUN'), (',', 'PUNCT'), ('некоторые', 'DET'), ('инструкции', 'NOUN'), ('должны', 'ADJ'), ('выполняться', 'VERB'), ('строго', 'ADV'), ('после', 'ADP'), ('завершения', 'NOUN'), ('работы', 'NOUN'), ('инструкций', 'NOUN'), (',', 'PUNCT'), ('от', 'ADP'), ('которых', 'PRON'), ('они', 'PRON'), ('зависят', 'VERB'), ('.', 'PUNCT')], [('Независимые', 'ADJ'), ('инструкции', 'NOUN'), ('или', 'CCONJ'), ('инструкции', 'NOUN'), (',', 'PUNCT'), ('ставшие', 'VERB'), ('независимыми', 'ADJ'), ('из-за', 'ADP'), ('завершения', 'NOUN'), ('работы', 'NOUN'), ('инструкций', 'NOUN'), (',', 'PUNCT'), ('от', 'ADP'), ('которых', 'PRON'), ('они', 'PRON'), ('зависят', 'VERB'), (',', 'PUNCT'), ('могут', 'VERB'), ('выполняться', 'VERB'), ('в', 'ADP'), ('произвольном', 'ADJ'), ('порядке', 'NOUN'), (',', 'PUNCT'), ('параллельно', 'ADV'), ('или', 'CCONJ'), ('одновременно', 'ADV'), (',', 'PUNCT'), ('если', 'SCONJ'), ('это', 'PRON'), ('позволяют', 'VERB'), ('используемые', 'VERB'), ('процессор', 'NOUN'), ('и', 'CCONJ'), ('операционная', 'ADJ'), ('система', 'NOUN'), ('.', 'PUNCT')], [('Ранее', 'ADV'), ('часто', 'ADV'), ('писали', 'VERB'), ('\"', 'PUNCT'), ('алгорифм', 'NOUN'), ('\"', 'PUNCT'), (',', 'PUNCT'), ('сейчас', 'ADV'), ('такое', 'DET'), ('написание', 'NOUN'), ('используется', 'VERB'), ('редко', 'ADV'), (',', 'PUNCT'), ('но', 'CCONJ'), (',', 'PUNCT'), ('тем', 'PRON'), ('не', 'PART'), ('менее', 'ADV'), (',', 'PUNCT'), ('имеет', 'VERB'), ('место', 'NOUN'), ('(', 'PUNCT'), ('например', 'ADV'), (',', 'PUNCT'), ('Нормальный', 'ADJ'), ('алгорифм', 'NOUN'), ('Маркова', 'PROPN'), (')', 'PUNCT'), ('.', 'PUNCT')], [('Часто', 'ADV'), ('в', 'ADP'), ('качестве', 'NOUN'), ('исполнителя', 'NOUN'), ('выступает', 'VERB'), ('некоторый', 'DET'), ('механизм', 'NOUN'), ('(', 'PUNCT'), ('компьютер', 'NOUN'), (',', 'PUNCT'), ('токарный', 'ADJ'), ('станок', 'NOUN'), (',', 'PUNCT'), ('швейная', 'ADJ'), ('машина', 'NOUN'), (')', 'PUNCT'), (',', 'PUNCT'), ('но', 'CCONJ'), ('понятие', 'NOUN'), ('алгоритма', 'NOUN'), ('необязательно', 'ADV'), ('относится', 'VERB'), ('к', 'ADP'), ('компьютерным', 'ADJ'), ('программам', 'NOUN'), (',', 'PUNCT'), ('так', 'ADV'), (',', 'PUNCT'), ('например', 'ADV'), (',', 'PUNCT'), ('чётко', 'ADV'), ('описанный', 'VERB'), ('рецепт', 'NOUN'), ('приготовления', 'NOUN'), ('блюда', 'NOUN'), ('также', 'ADV'), ('является', 'VERB'), ('алгоритмом', 'NOUN'), (',', 'PUNCT'), ('в', 'ADP'), ('таком', 'DET'), ('случае', 'NOUN'), ('исполнителем', 'NOUN'), ('является', 'VERB'), ('человек', 'NOUN'), ('.', 'PUNCT')], [('Определения', 'NOUN'), ('алгоритма', 'NOUN'), ('.', 'PUNCT')], [('Единого', 'ADJ'), ('\"', 'PUNCT'), ('истинного', 'ADJ'), ('\"', 'PUNCT'), ('определения', 'NOUN'), ('понятия', 'NOUN'), ('\"', 'PUNCT'), ('алгоритм', 'NOUN'), ('\"', 'PUNCT'), ('нет', 'VERB'), ('.', 'PUNCT')], [('\"', 'PUNCT'), ('Алгоритм', 'NOUN'), ('-', 'PUNCT'), ('это', 'PRON'), ('конечный', 'ADJ'), ('набор', 'NOUN'), ('правил', 'NOUN'), (',', 'PUNCT'), ('который', 'PRON'), ('определяет', 'VERB'), ('последовательность', 'NOUN'), ('операций', 'NOUN'), ('для', 'ADP'), ('решения', 'NOUN'), ('конкретного', 'ADJ'), ('множества', 'NOUN'), ('задач', 'NOUN'), ('и', 'CCONJ'), ('обладает', 'VERB'), ('пятью', 'NUM'), ('важными', 'ADJ'), ('чертами', 'NOUN'), (':', 'PUNCT'), ('конечность', 'NOUN'), (',', 'PUNCT'), ('определённость', 'NOUN'), (',', 'PUNCT'), ('ввод', 'NOUN'), (',', 'PUNCT'), ('вывод', 'NOUN'), (',', 'PUNCT'), ('эффективность', 'NOUN'), ('\"', 'PUNCT'), ('.', 'PUNCT'), ('(', 'PUNCT'), ('Д.', 'PROPN'), ('Э.', 'PROPN'), ('Кнут', 'PROPN'), (')', 'PUNCT'), ('.', 'PUNCT')], [('\"', 'PUNCT'), ('Алгоритм', 'NOUN'), ('-', 'PUNCT'), ('это', 'PRON'), ('всякая', 'DET'), ('система', 'NOUN'), ('вычислений', 'NOUN'), (',', 'PUNCT'), ('выполняемых', 'VERB'), ('по', 'ADP'), ('строго', 'ADV'), ('определённым', 'VERB'), ('правилам', 'NOUN'), (',', 'PUNCT'), ('которая', 'PRON'), ('после', 'ADP'), ('какого-либо', 'DET'), ('числа', 'NOUN'), ('шагов', 'NOUN'), ('заведомо', 'ADV'), ('приводит', 'VERB'), ('к', 'ADP'), ('решению', 'NOUN'), ('поставленной', 'VERB'), ('задачи', 'NOUN'), ('\"', 'PUNCT'), ('.', 'PUNCT'), ('(', 'PUNCT'), ('А.', 'PROPN'), ('Колмогоров', 'PROPN'), (')', 'PUNCT'), ('.', 'PUNCT')], [('\"', 'PUNCT'), ('Алгоритм', 'NOUN'), ('-', 'PUNCT'), ('это', 'PRON'), ('точное', 'ADJ'), ('предписание', 'NOUN'), (',', 'PUNCT'), ('определяющее', 'VERB'), ('вычислительный', 'ADJ'), ('процесс', 'NOUN'), (',', 'PUNCT'), ('идущий', 'VERB'), ('от', 'ADP'), ('варьируемых', 'VERB'), ('исходных', 'ADJ'), ('данных', 'NOUN'), ('к', 'ADP'), ('искомому', 'ADJ'), ('результату', 'NOUN'), ('\"', 'PUNCT'), ('.', 'PUNCT'), ('(', 'PUNCT'), ('А.', 'PROPN'), ('Марков', 'PROPN'), (')', 'PUNCT'), ('.', 'PUNCT')], [('\"', 'PUNCT'), ('Алгоритм', 'NOUN'), ('-', 'PUNCT'), ('точное', 'ADJ'), ('предписание', 'NOUN'), ('о', 'ADP'), ('выполнении', 'NOUN'), ('в', 'ADP'), ('определённом', 'ADJ'), ('порядке', 'NOUN'), ('некоторой', 'DET'), ('системы', 'NOUN'), ('операций', 'NOUN'), (',', 'PUNCT'), ('ведущих', 'VERB'), ('к', 'ADP'), ('решению', 'NOUN'), ('всех', 'DET'), ('задач', 'NOUN'), ('данного', 'ADJ'), ('типа', 'NOUN'), ('\"', 'PUNCT'), ('.', 'PUNCT'), ('(', 'PUNCT'), ('Философский', 'ADJ'), ('словарь', 'NOUN'), ('под', 'ADP'), ('ред.', 'NOUN'), ('М.', 'PROPN'), ('М.', 'PROPN'), ('Розенталя', 'PROPN'), (')', 'PUNCT'), ('.', 'PUNCT')], [('\"', 'PUNCT'), ('Алгоритм', 'NOUN'), ('-', 'PUNCT'), ('строго', 'ADV'), ('детерминированная', 'VERB'), ('последовательность', 'NOUN'), ('действий', 'NOUN'), (',', 'PUNCT'), ('описывающая', 'VERB'), ('процесс', 'NOUN'), ('преобразования', 'NOUN'), ('объекта', 'NOUN'), ('из', 'ADP'), ('начального', 'ADJ'), ('состояния', 'NOUN'), ('в', 'ADP'), ('конечное', 'ADJ'), (',', 'PUNCT'), ('записанная', 'VERB'), ('с', 'ADP'), ('помощью', 'NOUN'), ('понятных', 'ADJ'), ('исполнителю', 'NOUN'), ('команд', 'NOUN'), ('\"', 'PUNCT'), ('.', 'PUNCT'), ('(', 'PUNCT'), ('Николай', 'PROPN'), ('Дмитриевич', 'PROPN'), ('Угринович', 'PROPN'), (',', 'PUNCT'), ('учебник', 'NOUN'), ('\"', 'PUNCT'), ('Информатика', 'NOUN'), ('и', 'CCONJ'), ('информ', 'ADJ'), ('.', 'PUNCT'), ('технологии', 'NOUN'), ('\"', 'PUNCT'), (')', 'PUNCT'), ('.', 'PUNCT')], [('Формальные', 'ADJ'), ('свойства', 'NOUN'), ('алгоритмов', 'NOUN'), ('.', 'PUNCT')], [('Различные', 'ADJ'), ('определения', 'NOUN'), ('алгоритма', 'NOUN'), ('в', 'ADP'), ('явной', 'ADJ'), ('или', 'CCONJ'), ('неявной', 'ADJ'), ('форме', 'NOUN'), ('содержат', 'VERB'), ('следующий', 'ADJ'), ('ряд', 'NOUN'), ('общих', 'ADJ'), ('требований', 'NOUN'), (':', 'PUNCT')], [('-', 'PUNCT'), ('Дискретность', 'NOUN'), ('-', 'PUNCT'), ('алгоритм', 'NOUN'), ('должен', 'ADJ'), ('представлять', 'VERB'), ('процесс', 'NOUN'), ('решения', 'NOUN'), ('задачи', 'NOUN'), ('как', 'SCONJ'), ('последовательное', 'ADJ'), ('выполнение', 'NOUN'), ('некоторых', 'DET'), ('простых', 'ADJ'), ('шагов', 'NOUN'), ('.', 'PUNCT')], [('При', 'ADP'), ('этом', 'PRON'), ('для', 'ADP'), ('выполнения', 'NOUN'), ('каждого', 'DET'), ('шага', 'NOUN'), ('алгоритма', 'NOUN'), ('требуется', 'VERB'), ('конечный', 'ADJ'), ('отрезок', 'NOUN'), ('времени', 'NOUN'), (',', 'PUNCT'), ('то', 'PRON'), ('есть', 'VERB'), ('преобразование', 'NOUN'), ('исходных', 'ADJ'), ('данных', 'NOUN'), ('в', 'ADP'), ('результат', 'NOUN'), ('осуществляется', 'VERB'), ('во', 'ADP'), ('времени', 'NOUN'), ('дискретно', 'ADV'), ('.', 'PUNCT')], [('-', 'PUNCT'), ('Детерминированность', 'NOUN'), ('(', 'PUNCT'), ('определённость', 'NOUN'), (')', 'PUNCT'), ('.', 'PUNCT')], [('В', 'ADP'), ('каждый', 'DET'), ('момент', 'NOUN'), ('времени', 'NOUN'), ('следующий', 'ADJ'), ('шаг', 'NOUN'), ('работы', 'NOUN'), ('однозначно', 'ADV'), ('определяется', 'VERB'), ('состоянием', 'NOUN'), ('системы', 'NOUN'), ('.', 'PUNCT')]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbpEWbOZwDlF",
        "outputId": "85192819-2640-4a33-e6c0-320e7a1de00c"
      },
      "source": [
        "test_sent = conver_to_test_sent(full_test)[0]\n",
        "print(test_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Алгоритм', ',', 'от', 'имени', 'учёного', 'аль', '-', 'Хорезми', ',', '-', 'точный', 'набор', 'инструкций', ',', 'описывающих', 'порядок', 'действий', 'исполнителя', 'для', 'достижения', 'результата', 'решения', 'задачи', 'за', 'конечное', 'время', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "4S1PmPeFrLs-",
        "outputId": "cb0b7509-36d1-41e4-8b2d-4c79d6692c19"
      },
      "source": [
        "for sent in full_test[:2]:\n",
        "    for token in sent:\n",
        "        print(token.form, token.upos)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9d5771f21b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'news'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Conll' object has no attribute 'sents'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OshO48XLXQar"
      },
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import DefaultTagger\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.tag import BigramTagger, TrigramTagger\n",
        "from nltk.tag import RegexpTagger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj4tV8ytXTry",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "2bd10e3e-1d7e-46be-dcbb-abd2d00cac3e"
      },
      "source": [
        "default_tagger = nltk.DefaultTagger('NOUN')\n",
        "display(default_tagger.tag(test_sent), default_tagger.evaluate(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Алгоритм', 'NOUN'),\n",
              " (',', 'NOUN'),\n",
              " ('от', 'NOUN'),\n",
              " ('имени', 'NOUN'),\n",
              " ('учёного', 'NOUN'),\n",
              " ('аль', 'NOUN'),\n",
              " ('-', 'NOUN'),\n",
              " ('Хорезми', 'NOUN'),\n",
              " (',', 'NOUN'),\n",
              " ('-', 'NOUN'),\n",
              " ('точный', 'NOUN'),\n",
              " ('набор', 'NOUN'),\n",
              " ('инструкций', 'NOUN'),\n",
              " (',', 'NOUN'),\n",
              " ('описывающих', 'NOUN'),\n",
              " ('порядок', 'NOUN'),\n",
              " ('действий', 'NOUN'),\n",
              " ('исполнителя', 'NOUN'),\n",
              " ('для', 'NOUN'),\n",
              " ('достижения', 'NOUN'),\n",
              " ('результата', 'NOUN'),\n",
              " ('решения', 'NOUN'),\n",
              " ('задачи', 'NOUN'),\n",
              " ('за', 'NOUN'),\n",
              " ('конечное', 'NOUN'),\n",
              " ('время', 'NOUN'),\n",
              " ('.', 'NOUN')]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.23568564014423887"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "4Tn38Zusxth0",
        "outputId": "12f80d7b-b2e3-4b48-d7a2-4c4523851b66"
      },
      "source": [
        "unigram_tagger = UnigramTagger(train_data)\n",
        "display(unigram_tagger.tag(test_sent), unigram_tagger.evaluate(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Алгоритм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('имени', 'NOUN'),\n",
              " ('учёного', 'NOUN'),\n",
              " ('аль', 'PART'),\n",
              " ('-', 'PUNCT'),\n",
              " ('Хорезми', None),\n",
              " (',', 'PUNCT'),\n",
              " ('-', 'PUNCT'),\n",
              " ('точный', 'ADJ'),\n",
              " ('набор', 'NOUN'),\n",
              " ('инструкций', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('описывающих', 'VERB'),\n",
              " ('порядок', 'NOUN'),\n",
              " ('действий', 'NOUN'),\n",
              " ('исполнителя', 'NOUN'),\n",
              " ('для', 'ADP'),\n",
              " ('достижения', 'NOUN'),\n",
              " ('результата', 'NOUN'),\n",
              " ('решения', 'NOUN'),\n",
              " ('задачи', 'NOUN'),\n",
              " ('за', 'ADP'),\n",
              " ('конечное', None),\n",
              " ('время', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8772368820139521"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "F7_kPSsHzs_E",
        "outputId": "3d0a27f7-ea69-416f-c42d-96837ad9498a"
      },
      "source": [
        "bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n",
        "display(bigram_tagger.tag(test_sent), bigram_tagger.evaluate(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Алгоритм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('имени', 'NOUN'),\n",
              " ('учёного', 'NOUN'),\n",
              " ('аль', 'PART'),\n",
              " ('-', 'PUNCT'),\n",
              " ('Хорезми', None),\n",
              " (',', 'PUNCT'),\n",
              " ('-', 'PUNCT'),\n",
              " ('точный', 'ADJ'),\n",
              " ('набор', 'NOUN'),\n",
              " ('инструкций', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('описывающих', 'VERB'),\n",
              " ('порядок', 'NOUN'),\n",
              " ('действий', 'NOUN'),\n",
              " ('исполнителя', 'NOUN'),\n",
              " ('для', 'ADP'),\n",
              " ('достижения', 'NOUN'),\n",
              " ('результата', 'NOUN'),\n",
              " ('решения', 'NOUN'),\n",
              " ('задачи', 'NOUN'),\n",
              " ('за', 'ADP'),\n",
              " ('конечное', None),\n",
              " ('время', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8829996966939642"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "i5Rla51wzw-g",
        "outputId": "15ac16b5-5c27-4ac3-8a6e-5f6584f5f278"
      },
      "source": [
        "trigram_tagger = TrigramTagger(train_data, backoff=bigram_tagger)\n",
        "display(trigram_tagger.tag(test_sent), trigram_tagger.evaluate(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Алгоритм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('имени', 'NOUN'),\n",
              " ('учёного', 'NOUN'),\n",
              " ('аль', 'PART'),\n",
              " ('-', 'PUNCT'),\n",
              " ('Хорезми', None),\n",
              " (',', 'PUNCT'),\n",
              " ('-', 'PUNCT'),\n",
              " ('точный', 'ADJ'),\n",
              " ('набор', 'NOUN'),\n",
              " ('инструкций', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('описывающих', 'VERB'),\n",
              " ('порядок', 'NOUN'),\n",
              " ('действий', 'NOUN'),\n",
              " ('исполнителя', 'NOUN'),\n",
              " ('для', 'ADP'),\n",
              " ('достижения', 'NOUN'),\n",
              " ('результата', 'NOUN'),\n",
              " ('решения', 'NOUN'),\n",
              " ('задачи', 'NOUN'),\n",
              " ('за', 'ADP'),\n",
              " ('конечное', None),\n",
              " ('время', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8820982037542547"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdxLCN5y0AgP",
        "outputId": "02164d40-4dd2-40b6-a9a0-49e57611361b"
      },
      "source": [
        "from nltk.tag import TrigramTagger \n",
        "\n",
        "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
        "    for cls in tagger_classes:\n",
        "        backoff = cls(train_sents, backoff=backoff)\n",
        "    return backoff\n",
        "\n",
        "\n",
        "backoff = DefaultTagger('NN') \n",
        "tag = backoff_tagger(train_data,  \n",
        "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
        "                     backoff = backoff) \n",
        "  \n",
        "tag.evaluate(test_data) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8814915916826744"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0PteFwA3UFW"
      },
      "source": [
        "fdata_train = []\n",
        "for sent in full_train[:]:\n",
        "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_sent_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_sent_test.append([token.form for token in sent])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEAaYfI_3Woy",
        "outputId": "626cace6-a02a-4e1e-ecee-7b86f7ba27b7"
      },
      "source": [
        "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
        "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
        "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
        "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Наибольшая длина предложения 205\n",
            "Наибольшая длина токена 47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGa7GF3Y3YNx",
        "outputId": "4afd01b5-2b5f-436d-e200-19838c035eef"
      },
      "source": [
        "all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
        "all_test_texts = [' '.join(token.form for token in sent) for sent in full_test]\n",
        "\n",
        "all_train_labels = [' '.join(token.form for token in sent) for sent in full_train]\n",
        "all_test_labels = [' '.join(token.form for token in sent) for sent in full_test]\n",
        "print('\\n'.join(all_train_texts[:10]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Анкета .\n",
            "Начальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\n",
            "В приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\n",
            "Однако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\n",
            "Приемная была обставлена просто , но по-деловому .\n",
            "У двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\n",
            "В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\n",
            "Кабинет отличался скромностью , присущей Семену Еремеевичу .\n",
            "В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\n",
            "Справа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5ybtLJC3asR"
      },
      "source": [
        "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOD356Xp3dyL"
      },
      "source": [
        "train_tok = []\n",
        "train_label = []\n",
        "for sent in fdata_train[:]:\n",
        "    for tok in sent:\n",
        "        train_tok.append(tok[0])\n",
        "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
        "        \n",
        "test_tok = []\n",
        "test_label = []\n",
        "for sent in fdata_test[:]:\n",
        "    for tok in sent:\n",
        "        test_tok.append(tok[0])\n",
        "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp-vsxMV3fGi"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKsdeKjf3gVe"
      },
      "source": [
        "le = LabelEncoder()\n",
        "train_enc_labels = le.fit_transform(train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYAHFT4t3hzV"
      },
      "source": [
        "test_enc_labels = le.transform(test_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksUWt93w3jLK"
      },
      "source": [
        "hvectorizer = HashingVectorizer(ngram_range=(1, 5), analyzer='char_wb', n_features=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFCmLqxpGcF7"
      },
      "source": [
        "tfidvect = TfidfVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0ocGd3Z3kYL"
      },
      "source": [
        "X_train = tfidvect.fit_transform(train_tok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71owNabD3lY3"
      },
      "source": [
        "X_test = tfidvect.transform(test_tok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnepFZMf3m24",
        "outputId": "f13cd553-eab8-4291-f649-e6e530de685f"
      },
      "source": [
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(X_train, train_enc_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK-SIMZC3oT8"
      },
      "source": [
        "pred = lr.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmpIu3z-3pbj",
        "outputId": "1eec96c8-1790-4cbc-a5a7-fcafbc374c9c"
      },
      "source": [
        "accuracy_score(test_enc_labels, pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6640380143564857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJh48rF1BiEz"
      },
      "source": [
        "\n",
        "0.685454790550332 - char \\\\\n",
        "0.288326087689145 - word 1-5 \\\\\n",
        "0.23799413608330805 - word 2-3 \\\\\n",
        "0.7440518316314495 - char_wb 2-3 \\\\\n",
        "0.7333265931992047 - char_wb 1-5\n",
        "\n",
        "TfidfVectorizer \\\\\n",
        "0.6640380143564857\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu-NT0BsBeJi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIYrcobS-vzX"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cINqgGpKXURp"
      },
      "source": [
        "# Задание 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCM0drjKXYet"
      },
      "source": [
        "много дополнительных датасетов на русском языке\n",
        "\n",
        "https://natasha.github.io/corus/  \n",
        "https://github.com/natasha/corus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUOg4C8sZNpw"
      },
      "source": [
        "мы будем использовать данные http://ai-center.botik.ru/Airec/index.php/ru/collections/28-persons-1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzi6ApNLZg6X"
      },
      "source": [
        "**Проверить насколько хорошо работает NER**\n",
        "\n",
        "1. взять нер из nltk\n",
        "2. проверить deeppavlov\n",
        "3. написать свой нер попробовать разные подходы (с доп информацией без) так же с учётом соседей и без них\n",
        "4. сделать выводы по вашим экспериментам какой из подходов успешнее справляется"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP1LgaNUtaOz"
      },
      "source": [
        "при обучении своего нера незабудьте разделить выборку"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg6tcss2Zhp9",
        "outputId": "4f22ddc4-7cd4-493a-f2d1-3a7809a0a645"
      },
      "source": [
        "!pip install corus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting corus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/a3/e680679c669b0118271ac7246549c75a7088ab0e6696a3561408a3f9b50d/corus-0.9.0-py3-none-any.whl (83kB)\n",
            "\r\u001b[K     |████                            | 10kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 30kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 40kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 51kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 61kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 81kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: corus\n",
            "Successfully installed corus-0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrc5ocDkaS1e"
      },
      "source": [
        "import corus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPG7VIZJbH76",
        "outputId": "117c1207-21e2-437c-f4da-373e4c0ecfcf"
      },
      "source": [
        "!wget http://ai-center.botik.ru/Airec/ai-resources/Persons-1000.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-11 12:21:20--  http://ai-center.botik.ru/Airec/ai-resources/Persons-1000.zip\n",
            "Resolving ai-center.botik.ru (ai-center.botik.ru)... 95.129.138.2\n",
            "Connecting to ai-center.botik.ru (ai-center.botik.ru)|95.129.138.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3363777 (3.2M) [application/zip]\n",
            "Saving to: ‘Persons-1000.zip’\n",
            "\n",
            "Persons-1000.zip    100%[===================>]   3.21M  2.27MB/s    in 1.4s    \n",
            "\n",
            "2021-04-11 12:21:22 (2.27 MB/s) - ‘Persons-1000.zip’ saved [3363777/3363777]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmGWX23KbUfI",
        "outputId": "17b6afb8-bc25-4404-ae4b-f7214dc062ce"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets  Persons-1000.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmJr9tubbTVk"
      },
      "source": [
        "path = 'Persons-1000.zip'\n",
        "records = corus.persons.load_persons(path)\n",
        "rec = next(records)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzQutiCephkx",
        "outputId": "d0c5fe57-2908-404a-e5d7-ee467663d554"
      },
      "source": [
        "rec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PersonsMarkup(\n",
              "    text='Россия рассчитывает на конструктивное воздействие США на Грузию\\r\\n\\r\\n04/08/2008 12:08\\r\\n\\r\\nМОСКВА, 4 авг - РИА Новости. Россия рассчитывает, что США воздействуют на Тбилиси в связи с обострением ситуации в зоне грузино-осетинского конфликта. Об этом статс-секретарь - заместитель министра иностранных дел России Григорий Карасин заявил в телефонном разговоре с заместителем госсекретаря США Дэниэлом Фридом.\\r\\n\\r\\n\"С российской стороны выражена глубокая озабоченность в связи с новым витком напряженности вокруг Южной Осетии, противозаконными действиями грузинской стороны по наращиванию своих вооруженных сил в регионе, бесконтрольным строительством фортификационных сооружений\", - говорится в сообщении.\\r\\n\\r\\n\"Россия уже призвала Тбилиси к ответственной линии и рассчитывает также на конструктивное воздействие со стороны Вашингтона\", - сообщил МИД России. ',\n",
              "    spans=[PersonsSpan(\n",
              "         id=1,\n",
              "         start=308,\n",
              "         stop=324,\n",
              "         value='ГРИГОРИЙ КАРАСИН'\n",
              "     ), PersonsSpan(\n",
              "         id=2,\n",
              "         start=387,\n",
              "         stop=402,\n",
              "         value='ДЭНИЭЛ ФРИД'\n",
              "     )]\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utanZXahn-2I",
        "outputId": "c7141440-c196-479e-a979-9da2affd80f9"
      },
      "source": [
        "print(rec.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Россия рассчитывает на конструктивное воздействие США на Грузию\r\n",
            "\r\n",
            "04/08/2008 12:08\r\n",
            "\r\n",
            "МОСКВА, 4 авг - РИА Новости. Россия рассчитывает, что США воздействуют на Тбилиси в связи с обострением ситуации в зоне грузино-осетинского конфликта. Об этом статс-секретарь - заместитель министра иностранных дел России Григорий Карасин заявил в телефонном разговоре с заместителем госсекретаря США Дэниэлом Фридом.\r\n",
            "\r\n",
            "\"С российской стороны выражена глубокая озабоченность в связи с новым витком напряженности вокруг Южной Осетии, противозаконными действиями грузинской стороны по наращиванию своих вооруженных сил в регионе, бесконтрольным строительством фортификационных сооружений\", - говорится в сообщении.\r\n",
            "\r\n",
            "\"Россия уже призвала Тбилиси к ответственной линии и рассчитывает также на конструктивное воздействие со стороны Вашингтона\", - сообщил МИД России. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBaxNLbQoucW",
        "outputId": "0a5b0500-fd33-4e72-a10b-c44a9160e634"
      },
      "source": [
        "rec.spans"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PersonsSpan(\n",
              "     id=1,\n",
              "     start=308,\n",
              "     stop=324,\n",
              "     value='ГРИГОРИЙ КАРАСИН'\n",
              " ), PersonsSpan(\n",
              "     id=2,\n",
              "     start=387,\n",
              "     stop=402,\n",
              "     value='ДЭНИЭЛ ФРИД'\n",
              " )]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku7z5W_mnpFF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oRyOTnGtVSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c4d068f-03ce-41e9-f56c-8a7cb5644140"
      },
      "source": [
        "!pip install razdel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting razdel\n",
            "  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv4GhkdztVNC"
      },
      "source": [
        "from razdel import tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogf5-MIR9Dnu"
      },
      "source": [
        "records = corus.persons.load_persons(path)\n",
        "words_docs = []\n",
        "for ix, rec in enumerate(records):\n",
        "    words = []\n",
        "    for token in tokenize(rec.text):\n",
        "        is_person = False\n",
        "        for person in rec.spans:\n",
        "            if (token.start >= person.start) and (token.stop <= person.stop):\n",
        "                is_person = True\n",
        "                break\n",
        "        words.append([token.text, 'PERSON' if is_person else 'NO_PERSON'])\n",
        "    words_docs.extend(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELOiCSmY-rfj"
      },
      "source": [
        "теперь у нас есть слово и тег персона или нет, уже можно что-то делать(пробовать какие-то мл алгоритмы)  \n",
        "*дополнительно попробуйте проанализировать все ли токены если рассматривать униграмный подход относятся к тегам персона с логической точки зрения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMg8h9tO-kV0",
        "outputId": "58134e05-83f4-4b8e-b612-2c6e32ba4c99"
      },
      "source": [
        "words_docs[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Россия', 'NO_PERSON'],\n",
              " ['рассчитывает', 'NO_PERSON'],\n",
              " ['на', 'NO_PERSON'],\n",
              " ['конструктивное', 'NO_PERSON'],\n",
              " ['воздействие', 'NO_PERSON']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbTb2e45IlzQ",
        "outputId": "5de3b432-0779-439c-ae22-b40e2efddd11"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xtam1-3-kMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8adbd218-4f78-46b0-8529-06bcbf272650"
      },
      "source": [
        "nltk.pos_tag(nltk.word_tokenize(rec.text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Мать', 'JJ'),\n",
              " ('рядового', 'NNP'),\n",
              " ('Владислава', 'NNP'),\n",
              " ('Челаха', 'NNP'),\n",
              " (',', ','),\n",
              " ('обвиняемого', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('убийстве', 'VBD'),\n",
              " ('14', 'CD'),\n",
              " ('пограничников', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('егеря', 'NNP'),\n",
              " (',', ','),\n",
              " ('после', 'NNP'),\n",
              " ('свидания', 'NNP'),\n",
              " ('с', 'NNP'),\n",
              " ('сыном', 'NNP'),\n",
              " ('заметила', 'NNP'),\n",
              " ('следы', 'NNP'),\n",
              " ('побоев', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Как', 'VB'),\n",
              " ('передаёт', 'JJ'),\n",
              " ('РИА', 'NNP'),\n",
              " ('``', '``'),\n",
              " ('Новости', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " ('со', 'CC'),\n",
              " ('ссылкой', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('местную', 'NNP'),\n",
              " ('газету', 'NNP'),\n",
              " ('``', '``'),\n",
              " ('Время', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " (',', ','),\n",
              " ('Владислав', 'NNP'),\n",
              " ('Челах', 'NNP'),\n",
              " (',', ','),\n",
              " ('несший', 'NNP'),\n",
              " ('службу', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('пограничной', 'NNP'),\n",
              " ('заставе', 'NNP'),\n",
              " ('``', '``'),\n",
              " ('Арканкерген', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " ('в', 'CC'),\n",
              " ('Алма-Атинской', 'JJ'),\n",
              " ('области', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('границе', 'NNP'),\n",
              " ('с', 'NNP'),\n",
              " ('Китаем', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('подозреваемый', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('убийстве', 'VBD'),\n",
              " ('14', 'CD'),\n",
              " ('сослуживцев', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('егеря', 'NNP'),\n",
              " (',', ','),\n",
              " ('заявил', 'NNP'),\n",
              " ('своей', 'NNP'),\n",
              " ('матери', 'NNP'),\n",
              " ('во', 'NNP'),\n",
              " ('время', 'NNP'),\n",
              " ('свидания', 'NNP'),\n",
              " (',', ','),\n",
              " ('что', 'NNP'),\n",
              " ('никого', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('убивал', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Сама', 'VB'),\n",
              " ('женщина', 'JJ'),\n",
              " ('заявила', 'NNP'),\n",
              " (',', ','),\n",
              " ('что', 'NNP'),\n",
              " ('встречаться', 'NNP'),\n",
              " ('с', 'NNP'),\n",
              " ('сыном', 'NNP'),\n",
              " ('ей', 'NNP'),\n",
              " ('пришлось', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('присутствии', 'NNP'),\n",
              " ('видеокамеры', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Ей', 'VB'),\n",
              " ('даже', 'JJ'),\n",
              " ('пришлось', 'NNP'),\n",
              " ('подписать', 'NNP'),\n",
              " ('согласие', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('это', 'NNP'),\n",
              " (',', ','),\n",
              " ('иначе', 'NNP'),\n",
              " ('бы', 'NNP'),\n",
              " ('свидание', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('состоялось', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('``', '``'),\n",
              " ('Меня', 'JJ'),\n",
              " ('завели', 'NN'),\n",
              " ('в', 'NNP'),\n",
              " ('маленькую', 'NNP'),\n",
              " ('комнатушку', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Следом', 'VB'),\n",
              " ('залетели', 'JJ'),\n",
              " ('несколько', 'NNP'),\n",
              " ('человек', 'NNP'),\n",
              " ('с', 'NNP'),\n",
              " ('видеокамерой', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Они', 'VB'),\n",
              " ('снимали', 'JJ'),\n",
              " ('нас', 'NNP'),\n",
              " ('все', 'NNP'),\n",
              " ('эти', 'VBD'),\n",
              " ('5', 'CD'),\n",
              " ('минут', 'NN'),\n",
              " (',', ','),\n",
              " ('что', 'NNP'),\n",
              " ('длилось', 'NNP'),\n",
              " ('свидание', 'NNP'),\n",
              " (',', ','),\n",
              " ('чтобы', 'NNP'),\n",
              " ('Влад', 'NNP'),\n",
              " ('ничего', 'NNP'),\n",
              " ('лишнего', 'NNP'),\n",
              " ('мне', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('сказал', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Я', 'NN'),\n",
              " ('расплакалась', 'NN'),\n",
              " (',', ','),\n",
              " ('подошла', 'NNP'),\n",
              " ('к', 'NNP'),\n",
              " ('сыну', 'NNP'),\n",
              " (',', ','),\n",
              " ('он', 'NNP'),\n",
              " ('обнял', 'NNP'),\n",
              " ('меня', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('шепнул', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('ухо', 'NN'),\n",
              " (':', ':'),\n",
              " ('``', '``'),\n",
              " ('Мам', 'NN'),\n",
              " (',', ','),\n",
              " ('я', 'NNP'),\n",
              " ('этого', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('делал', 'NN'),\n",
              " ('!', '.'),\n",
              " (\"''\", \"''\"),\n",
              " ('-', ':'),\n",
              " ('цитирует', 'NN'),\n",
              " ('издание', 'JJ'),\n",
              " ('мать', 'NNP'),\n",
              " ('пограничника', 'NNP'),\n",
              " ('Светлану', 'NNP'),\n",
              " ('Ващенко', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Кроме', 'NN'),\n",
              " ('того', 'NN'),\n",
              " (',', ','),\n",
              " ('по', 'NNP'),\n",
              " ('словам', 'NNP'),\n",
              " ('женщины', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('щеке', 'NNP'),\n",
              " ('сына', 'NNP'),\n",
              " ('Светлана', 'NNP'),\n",
              " ('увидела', 'NNP'),\n",
              " ('синяк', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Как', 'VB'),\n",
              " ('рассказал', 'JJ'),\n",
              " ('дед', 'NNP'),\n",
              " ('рядового', 'NNP'),\n",
              " ('Владимир', 'NNP'),\n",
              " ('Челах', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('пресс-конференции', 'NNP'),\n",
              " (',', ','),\n",
              " ('состоявшейся', 'NNP'),\n",
              " ('после', 'NNP'),\n",
              " ('свидания', 'NNP'),\n",
              " ('с', 'NNP'),\n",
              " ('обвиняемым', 'NNP'),\n",
              " (',', ','),\n",
              " ('во', 'NNP'),\n",
              " ('время', 'NNP'),\n",
              " ('встречи', 'NNP'),\n",
              " ('его', 'NNP'),\n",
              " ('внук', 'NNP'),\n",
              " ('почти', 'NNP'),\n",
              " ('ничего', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('говорил', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('``', '``'),\n",
              " ('Он', 'JJ'),\n",
              " ('ничего', 'NN'),\n",
              " ('не', 'NNP'),\n",
              " ('мог', 'NNP'),\n",
              " ('сказать', 'NNP'),\n",
              " (',', ','),\n",
              " ('он', 'NNP'),\n",
              " ('до', 'NNP'),\n",
              " ('сих', 'NNP'),\n",
              " ('пор', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('шоке', 'NNP'),\n",
              " (',', ','),\n",
              " ('видимо', 'NNP'),\n",
              " (',', ','),\n",
              " ('он', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('ожидал', 'NNP'),\n",
              " (',', ','),\n",
              " ('что', 'NNP'),\n",
              " ('к', 'NNP'),\n",
              " ('нему', 'NNP'),\n",
              " ('приедут', 'NNP'),\n",
              " ('родные', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Ничего', 'VB'),\n",
              " ('сказать', 'JJ'),\n",
              " ('не', 'NNP'),\n",
              " ('смог', 'NNP'),\n",
              " (',', ','),\n",
              " ('на', 'NNP'),\n",
              " ('вопросы', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('ответил', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('И', 'VB'),\n",
              " ('только', 'JJ'),\n",
              " ('когда', 'NNP'),\n",
              " ('мы', 'NNP'),\n",
              " ('обнялись', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('прощание', 'NNP'),\n",
              " (',', ','),\n",
              " ('у', 'NNP'),\n",
              " ('него', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('глазах', 'NNP'),\n",
              " ('что-то', 'JJ'),\n",
              " ('промелькнуло', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Я', 'NN'),\n",
              " ('думаю', 'NN'),\n",
              " (',', ','),\n",
              " ('он', 'NNP'),\n",
              " ('меня', 'NNP'),\n",
              " ('услышал', 'NNP'),\n",
              " (\"''\", \"''\"),\n",
              " (',', ','),\n",
              " ('-', ':'),\n",
              " ('цитирует', 'NN'),\n",
              " ('Челаха', 'JJ'),\n",
              " ('BNews.kz', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Владимир', 'VB'),\n",
              " ('Челах', 'JJ'),\n",
              " ('также', 'NNP'),\n",
              " ('заявил', 'NNP'),\n",
              " (',', ','),\n",
              " ('что', 'NNP'),\n",
              " ('родственники', 'NNP'),\n",
              " ('пограничников', 'NNP'),\n",
              " (',', ','),\n",
              " ('погибших', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('``', '``'),\n",
              " ('Арканкергене', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " (',', ','),\n",
              " ('уверены', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('невиновности', 'NNP'),\n",
              " ('его', 'NNP'),\n",
              " ('внука', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('``', '``'),\n",
              " ('Они', 'JJ'),\n",
              " ('каждый', 'NN'),\n",
              " ('день', 'NNP'),\n",
              " ('звонят', 'NNP'),\n",
              " ('нам', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('спрашивают', 'NNP'),\n",
              " ('о', 'NNP'),\n",
              " ('нашем', 'NNP'),\n",
              " ('состоянии', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Родители', 'VB'),\n",
              " ('погибших', 'JJ'),\n",
              " ('солдат', 'NNP'),\n",
              " ('уверены', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('невиновности', 'NNP'),\n",
              " ('моего', 'NNP'),\n",
              " ('внука', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Они', 'VB'),\n",
              " ('готовы', 'JJ'),\n",
              " ('поддержать', 'NNP'),\n",
              " ('нашу', 'NNP'),\n",
              " ('семью', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Пистолет', 'NN'),\n",
              " (',', ','),\n",
              " ('который', 'NNP'),\n",
              " ('якобы', 'NNP'),\n",
              " ('нашли', 'NNP'),\n",
              " ('у', 'NNP'),\n",
              " ('моего', 'NNP'),\n",
              " ('внука', 'NNP'),\n",
              " (',', ','),\n",
              " ('до', 'NNP'),\n",
              " ('сих', 'NNP'),\n",
              " ('пор', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('отправлен', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('экспертизу', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Когда', 'VB'),\n",
              " ('умывали', 'JJ'),\n",
              " ('одного', 'NNP'),\n",
              " ('из', 'NNP'),\n",
              " ('солдат', 'NNP'),\n",
              " (',', ','),\n",
              " ('родители', 'NNP'),\n",
              " ('обнаружили', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('нем', 'NNP'),\n",
              " ('три', 'NNP'),\n",
              " ('ножевых', 'NNP'),\n",
              " ('ранения', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('множество', 'NNP'),\n",
              " ('гематом', 'NNP'),\n",
              " (\"''\", \"''\"),\n",
              " (',', ','),\n",
              " ('-', ':'),\n",
              " ('заявил', 'NN'),\n",
              " ('Челах', 'NN'),\n",
              " ('.', '.'),\n",
              " ('Напомним', 'NN'),\n",
              " (',', ','),\n",
              " ('на', 'NNP'),\n",
              " ('казахстанском', 'NNP'),\n",
              " ('пограничном', 'NNP'),\n",
              " ('временном', 'NNP'),\n",
              " ('посту', 'NNP'),\n",
              " ('``', '``'),\n",
              " ('Арканкерген', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " ('были', 'CC'),\n",
              " ('обнаружены', 'NNP'),\n",
              " ('обгоревшие', 'NNP'),\n",
              " ('тела', 'VBD'),\n",
              " ('14', 'CD'),\n",
              " ('пограничников', 'NNP'),\n",
              " ('и', 'VBD'),\n",
              " ('1', 'CD'),\n",
              " ('егеря', 'NN'),\n",
              " ('.', '.'),\n",
              " ('Спустя', 'CC'),\n",
              " ('несколько', 'JJ'),\n",
              " ('дней', 'NNP'),\n",
              " ('удалось', 'NNP'),\n",
              " ('задержать', 'NNP'),\n",
              " ('15-го', 'JJ'),\n",
              " ('солдата-срочника', 'JJ'),\n",
              " ('Владислава', 'NN'),\n",
              " ('Челаха', 'NN'),\n",
              " (',', ','),\n",
              " ('который', 'NNP'),\n",
              " (',', ','),\n",
              " ('по', 'NNP'),\n",
              " ('версии', 'NNP'),\n",
              " ('следствия', 'NNP'),\n",
              " (',', ','),\n",
              " ('является', 'NNP'),\n",
              " ('убийцей', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('поджигателем', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('В', 'VB'),\n",
              " ('качестве', 'JJ'),\n",
              " ('причины', 'NNP'),\n",
              " ('называются', 'NNP'),\n",
              " ('``', '``'),\n",
              " ('внутренние', 'NNP'),\n",
              " ('конфликты', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('необъяснимое', 'NNP'),\n",
              " ('состояние', 'NNP'),\n",
              " ('помутнения', 'NNP'),\n",
              " ('сознания', 'NNP'),\n",
              " (\"''\", \"''\"),\n",
              " ('.', '.'),\n",
              " ('Ранее', 'VB'),\n",
              " ('заявлялось', 'NN'),\n",
              " (',', ','),\n",
              " ('что', 'NNP'),\n",
              " ('Владислав', 'NNP'),\n",
              " ('сознался', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('преступлении', 'NNP'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv2ip1HMJURJ",
        "outputId": "db1544f9-c686-47e0-a3f3-97276a2d2fac"
      },
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w8H3ixZJWqE",
        "outputId": "111f38b9-0086-4b83-ee02-f55417875191"
      },
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92YoEJM6JQFX",
        "outputId": "2e9013fa-e49b-40f4-a128-dbf9c9070afb"
      },
      "source": [
        "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(rec.text))) if hasattr(chunk, 'label') }"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('Владислав Челах', 'PERSON'),\n",
              " ('Китаем', 'PERSON'),\n",
              " ('Кроме', 'PERSON'),\n",
              " ('Мать', 'PERSON'),\n",
              " ('Напомним', 'PERSON'),\n",
              " ('Пистолет', 'PERSON'),\n",
              " ('РИА', 'ORGANIZATION'),\n",
              " ('Челах', 'PERSON'),\n",
              " ('Челаха', 'PERSON'),\n",
              " ('Челаха BNews.kz', 'PERSON')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skYaNCiC5xM4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2kd-emBao1u-",
        "outputId": "ae4c80d4-a783-4ed9-d921-5de842da2d65"
      },
      "source": [
        "# установка deeppavlov\n",
        "\n",
        "!pip uninstall -y tensorflow tensorflow-gpu\n",
        "!pip install numpy scipy librosa unidecode inflect librosa transformers\n",
        "!pip install deeppavlov"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.1:\n",
            "  Successfully uninstalled tensorflow-2.4.1\n",
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 11.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.22.2.post1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 16.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 42.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (54.2.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa) (1.14.5)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa) (2.20)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=dda7dea04f22309cd00bc3f471204ab0b042ccd202d7a2362ee25735181deef5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: unidecode, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0 unidecode-1.2.0\n",
            "Collecting deeppavlov\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/fa/e77a41199168fcf3cd2b75a38b5985b7ced7aded0ff9422f7373385a583e/deeppavlov-0.14.1-py3-none-any.whl (988kB)\n",
            "\u001b[K     |████████████████████████████████| 993kB 8.7MB/s \n",
            "\u001b[?25hCollecting prometheus-client==0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/23/41a5a24b502d35a4ad50a5bb7202a5e1d9a0364d0c12f56db3dbf7aca76d/prometheus_client-0.7.1.tar.gz\n",
            "Collecting numpy==1.18.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/53/127cb49435bcf5d841baf8eafa030931c62a9eac577a641f8c2293d23371/numpy-1.18.0-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.5MB/s \n",
            "\u001b[?25hCollecting rusenttokenize==0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/25/4c/a2f00be5def774a3df2e5387145f1cb54e324607ec4a7e23f573645946e7/rusenttokenize-0.0.5-py3-none-any.whl\n",
            "Collecting pytelegrambotapi==3.6.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/ab/99c606f69fcda57e35788b913dd34c9d9acb48dd26349141b3855dcf6351/pyTelegramBotAPI-3.6.7.tar.gz (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.2MB/s \n",
            "\u001b[?25hCollecting pydantic==1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/56/1f652c3f658d2a9fd495d2e988a2da57eabdb6c4b8f4563c2ccbe6a2a8c5/pydantic-1.3-cp37-cp37m-manylinux2010_x86_64.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 20.6MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 37.2MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 34.5MB/s \n",
            "\u001b[?25hCollecting pyopenssl==19.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.0MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml==0.15.100\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/fc/12de89822adaa3a60b8cb0139bae75918278999d08e6dff158623abd7cba/ruamel.yaml-0.15.100-cp37-cp37m-manylinux1_x86_64.whl (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 36.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm==4.41.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (4.41.1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (1.4.1)\n",
            "Collecting Cython==0.29.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/58/2deb24de3c10cc4c0f09639b46f4f4b50059f0fdc785128a57dd9fdce026/Cython-0.29.14-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 40.4MB/s \n",
            "\u001b[?25hCollecting overrides==2.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/98/2430afd204c48ac0a529d439d7e22df8fa603c668d03456b5947cb59ec36/overrides-2.7.0.tar.gz\n",
            "Collecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.3MB/s \n",
            "\u001b[?25hCollecting pymorphy2==0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.0.12)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2.10.0)\n",
            "Collecting aio-pika==6.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/07/196a4115cbef31fa0c3dabdea146f02dffe5e49998341d20dbe2278953bc/aio_pika-6.4.1-py3-none-any.whl (40kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 268kB/s \n",
            "\u001b[?25hCollecting uvicorn==0.11.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/5f/2bc87272f189662e129ddcd4807ad3ef83128b4df3a3482335f5f9790f24/uvicorn-0.11.7-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.21.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/a4/a48bd4b0d15395362b561df7e7247de87291105eb736a3b2aaffebf437b9/scikit_learn-0.21.2-cp37-cp37m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 31.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (7.1.2)\n",
            "Collecting pytz==2019.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl (510kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 37.4MB/s \n",
            "\u001b[?25hCollecting uvloop==0.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/7a/54a80c03b555af21680a2f3692947b43a0d576d90c4c18cace0fee1ccc0e/uvloop-0.14.0-cp37-cp37m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 36.2MB/s \n",
            "\u001b[?25hCollecting fastapi==0.47.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/a7/4804d7abf8a1544d079d50650af872387154ebdac5bd07d54b2e60e2b334/fastapi-0.47.1-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.3MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 27.8MB/s \n",
            "\u001b[?25hCollecting pandas==0.25.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/e0/a1b39cdcb2c391f087a1538bc8a6d62a82d0439693192aef541d7b123769/pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 26.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pytelegrambotapi==3.6.7->deeppavlov) (1.15.0)\n",
            "Collecting cryptography>=2.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 37.9MB/s \n",
            "\u001b[?25hCollecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 26.2MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting aiormq<4,>=3.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0b/c4/dc5b9d50c15af2ee187974a5a0c3f20c06cce6559eea4c065d372e846b6a/aiormq-3.3.1-py3-none-any.whl\n",
            "Collecting yarl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 51.2MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.1MB/s \n",
            "\u001b[?25hCollecting websockets==8.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/0b/3ebc752392a368af14dd24ee041683416ac6d2463eead94b311b11e41c82/websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.8MB/s \n",
            "\u001b[?25hCollecting httptools==0.1.*; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/52/295101ea5a60f9bee805a3ca422863600ba5cac4e2778ac7bd56efab1231/httptools-0.1.1-cp37-cp37m-manylinux1_x86_64.whl (217kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 54.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.2->deeppavlov) (1.0.1)\n",
            "Collecting starlette<=0.12.9,>=0.12.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/95/2220fe5bf287e693a6430d8ee36c681b0157035b7249ec08f8fb36319d16/starlette-0.12.9.tar.gz (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.5)\n",
            "Collecting pamqp==2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/56/afa06143361e640c9159d828dadc95fc9195c52c95b4a97d136617b0166d/pamqp-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (3.7.4.3)\n",
            "Collecting multidict>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n",
            "Building wheels for collected packages: prometheus-client, pytelegrambotapi, nltk, overrides, sacremoses, starlette\n",
            "  Building wheel for prometheus-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-client: filename=prometheus_client-0.7.1-cp37-none-any.whl size=41404 sha256=b9b239c797114b39084277adfef1f6c71ee1a66e688b565e3f09d5934459c399\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/54/34/fd47cd9b308826cc4292b54449c1899a30251ef3b506bc91ea\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-cp37-none-any.whl size=47177 sha256=809288b245362cf6929716cbc2a3a510f4b65913bd5e4b5d104bf52ea3504c02\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/40/18/8a34153f95ef0dc19e3954898e5a5079244b76a8afdd7d0ec5\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449908 sha256=eee776ff6c3eb6666e2201ef9a2351f93f6699283338d353401a4f109d6281f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.7.0-cp37-none-any.whl size=5600 sha256=96679f26195aa9c9dd4e2f65071b8f04a6528e3ee0d30956645454af3184db19\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/7c/ef/80508418b67d87371c5b3de49e03eb22ee7c1d19affb5099f8\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp37-none-any.whl size=883999 sha256=bdbcd9e63b3485c11013d6338f47ab58a9a747e4a3cf75ce09fc13e4c12aaf3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for starlette: filename=starlette-0.12.9-cp37-none-any.whl size=57244 sha256=f694b2404fd69b3907912c744a7c240c551f1b62884d2e10bbf1fbea4f535a1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/51/5b/3828d52e185cafad941c4291b6f70894d0794be28c70addae5\n",
            "Successfully built prometheus-client pytelegrambotapi nltk overrides sacremoses starlette\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: prometheus-client, numpy, rusenttokenize, idna, requests, pytelegrambotapi, pydantic, pymorphy2-dicts-ru, nltk, cryptography, pyopenssl, ruamel.yaml, Cython, overrides, pymorphy2-dicts, dawg-python, pymorphy2, multidict, yarl, pamqp, aiormq, aio-pika, h11, uvloop, websockets, httptools, uvicorn, scikit-learn, pytz, starlette, fastapi, sacremoses, pandas, deeppavlov\n",
            "  Found existing installation: prometheus-client 0.10.0\n",
            "    Uninstalling prometheus-client-0.10.0:\n",
            "      Successfully uninstalled prometheus-client-0.10.0\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: Cython 0.29.22\n",
            "    Uninstalling Cython-0.29.22:\n",
            "      Successfully uninstalled Cython-0.29.22\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Found existing installation: sacremoses 0.0.44\n",
            "    Uninstalling sacremoses-0.0.44:\n",
            "      Successfully uninstalled sacremoses-0.0.44\n",
            "  Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "Successfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.3.1 cryptography-3.4.7 dawg-python-0.7.2 deeppavlov-0.14.1 fastapi-0.47.1 h11-0.9.0 httptools-0.1.1 idna-2.8 multidict-5.1.0 nltk-3.4.5 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pandas-0.25.3 prometheus-client-0.7.1 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.417127.4579844 pyopenssl-19.1.0 pytelegrambotapi-3.6.7 pytz-2019.1 requests-2.22.0 ruamel.yaml-0.15.100 rusenttokenize-0.0.5 sacremoses-0.0.35 scikit-learn-0.21.2 starlette-0.12.9 uvicorn-0.11.7 uvloop-0.14.0 websockets-8.1 yarl-1.6.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "idna",
                  "nltk",
                  "numpy",
                  "pandas",
                  "pytz",
                  "requests",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY96lqBzsZJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b7102d-bd97-48fd-9bc2-3caa36704db5"
      },
      "source": [
        "!python -m deeppavlov install squad_bert\n",
        "!python -m deeppavlov install ner_ontonotes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-11 13:31:18.471 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'squad_bert' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/squad/squad_bert.json'\n",
            "Collecting tensorflow==1.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/81/84fb7a323f9723f81edfc796d89e89aa95a9446ed7353c144195b3a3a3ba/tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 89kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.36.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.18.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 33.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.12.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 22.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.2) (54.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=a4555c7da9ab2261646e9ee05b89798954edeb8a2f242be1578268c73488c7a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorflow-estimator, keras-applications, tensorboard, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n",
            "Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
            "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-tbfdyjph\n",
            "  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-tbfdyjph\n",
            "Building wheels for collected packages: bert-dp\n",
            "  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-dp: filename=bert_dp-1.0-cp37-none-any.whl size=23581 sha256=65e66a7f23534c0fe50601dab7c2166bf30e7e82e84dea16352bbbb090fbfb14\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bccif7j_/wheels/1e/41/94/886107eaf932532594886fd8bfc9cb9d4db632e94add49d326\n",
            "Successfully built bert-dp\n",
            "Installing collected packages: bert-dp\n",
            "Successfully installed bert-dp-1.0\n",
            "2021-04-11 13:31:58.390 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'ner_ontonotes' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/ner/ner_ontonotes.json'\n",
            "Requirement already satisfied: tensorflow==1.15.2 in /usr/local/lib/python3.7/dist-packages (1.15.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.18.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.12.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.32.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.2) (54.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
            "Collecting gensim==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/93/c6011037f24e3106d13f3be55297bf84ece2bf15b278cc4776339dc52db5/gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (4.2.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.18.0)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KE7tpVWs1b7"
      },
      "source": [
        "import deeppavlov\n",
        "from deeppavlov import configs, build_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsegbgCbrzy_",
        "outputId": "ac461ce9-7feb-4993-9085-76b54d74c0f4"
      },
      "source": [
        "deeppavlov_ner = build_model(configs.ner.ner_bert_ent_and_type_rus, download=True)\n",
        "rus_document = \"Нью-Йорк, США, 30 апреля 2020, 01:01 — REGNUM В администрации президента США Дональда Трампа планируют пройти все этапы создания вакцины от коронавируса в ускоренном темпе и выпустить 100 млн доз до конца 2020 года, передаёт агентство Bloomberg со ссылкой на осведомлённые источники\"\n",
        "deeppavlov_ner([rus_document])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-11 13:33:48.677 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/kbqa/models/ner_cq_rus.tar.gz download because of matching hashes\n",
            "2021-04-11 13:34:02.612 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/multi_cased_L-12_H-768_A-12.zip download because of matching hashes\n",
            "2021-04-11 13:34:03.705 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/kbqa/datasets/entity_and_type_detection_rus.pickle download because of matching hashes\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-11 13:34:05.768 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/ner_ent_and_type_rus/tag.dict]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:236: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:314: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
            "\n",
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:75: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:571: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:94: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:671: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:249: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-11 13:34:36.325 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/ner_ent_and_type_rus/model]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/ner_ent_and_type_rus/model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['Нью',\n",
              "   '-',\n",
              "   'Йорк',\n",
              "   ',',\n",
              "   'США',\n",
              "   ',',\n",
              "   '30',\n",
              "   'апреля',\n",
              "   '2020',\n",
              "   ',',\n",
              "   '01',\n",
              "   ':',\n",
              "   '01',\n",
              "   '—',\n",
              "   'REGNUM',\n",
              "   'В',\n",
              "   'администрации',\n",
              "   'президента',\n",
              "   'США',\n",
              "   'Дональда',\n",
              "   'Трампа',\n",
              "   'планируют',\n",
              "   'пройти',\n",
              "   'все',\n",
              "   'этапы',\n",
              "   'создания',\n",
              "   'вакцины',\n",
              "   'от',\n",
              "   'коронавируса',\n",
              "   'в',\n",
              "   'ускоренном',\n",
              "   'темпе',\n",
              "   'и',\n",
              "   'выпустить',\n",
              "   '100',\n",
              "   'млн',\n",
              "   'доз',\n",
              "   'до',\n",
              "   'конца',\n",
              "   '2020',\n",
              "   'года',\n",
              "   ',',\n",
              "   'передаёт',\n",
              "   'агентство',\n",
              "   'Bloomberg',\n",
              "   'со',\n",
              "   'ссылкой',\n",
              "   'на',\n",
              "   'осведомлённые',\n",
              "   'источники']],\n",
              " array([[[9.71761167e-01, 2.64181718e-02, 1.82059826e-03],\n",
              "         [9.60046053e-01, 3.84359621e-02, 1.51804090e-03],\n",
              "         [9.21502233e-01, 7.51483515e-02, 3.34933866e-03],\n",
              "         [9.92245018e-01, 6.89126039e-03, 8.63717811e-04],\n",
              "         [9.87261832e-01, 1.17831351e-02, 9.55077179e-04],\n",
              "         [9.96919036e-01, 2.74484046e-03, 3.36058962e-04],\n",
              "         [9.97517467e-01, 2.30196840e-03, 1.80645322e-04],\n",
              "         [9.95577574e-01, 4.12209751e-03, 3.00363230e-04],\n",
              "         [9.91355956e-01, 8.05664156e-03, 5.87437069e-04],\n",
              "         [9.98594940e-01, 1.27561530e-03, 1.29398570e-04],\n",
              "         [9.98789489e-01, 1.13862741e-03, 7.19211675e-05],\n",
              "         [9.97291386e-01, 2.53513828e-03, 1.73508219e-04],\n",
              "         [9.98374701e-01, 1.53756991e-03, 8.77227576e-05],\n",
              "         [9.97647226e-01, 2.15254142e-03, 2.00248949e-04],\n",
              "         [9.96125996e-01, 3.52644222e-03, 3.47553432e-04],\n",
              "         [9.98647392e-01, 1.18941162e-03, 1.63180783e-04],\n",
              "         [9.94467735e-01, 4.99971071e-03, 5.32494334e-04],\n",
              "         [9.88695621e-01, 1.04803387e-02, 8.23963608e-04],\n",
              "         [9.90075767e-01, 8.98968987e-03, 9.34486161e-04],\n",
              "         [1.60550028e-01, 8.38216543e-01, 1.23350031e-03],\n",
              "         [1.39164224e-01, 8.59113455e-01, 1.72229693e-03],\n",
              "         [9.98087585e-01, 1.73141155e-03, 1.80948962e-04],\n",
              "         [9.98838842e-01, 1.03886123e-03, 1.22317724e-04],\n",
              "         [9.98723567e-01, 1.15420937e-03, 1.22209996e-04],\n",
              "         [9.97251451e-01, 2.43649492e-03, 3.12116492e-04],\n",
              "         [9.96265590e-01, 3.32576130e-03, 4.08622727e-04],\n",
              "         [9.88883674e-01, 1.00314673e-02, 1.08485715e-03],\n",
              "         [9.81114209e-01, 1.81934275e-02, 6.92377449e-04],\n",
              "         [3.06576163e-01, 6.90057814e-01, 3.36598023e-03],\n",
              "         [9.96585727e-01, 3.16357287e-03, 2.50688347e-04],\n",
              "         [9.96187031e-01, 3.55201564e-03, 2.60923203e-04],\n",
              "         [9.94267046e-01, 5.20009315e-03, 5.32936596e-04],\n",
              "         [9.98957992e-01, 9.21283732e-04, 1.20748897e-04],\n",
              "         [9.98963594e-01, 9.19635117e-04, 1.16777301e-04],\n",
              "         [9.94871259e-01, 4.77636745e-03, 3.52344301e-04],\n",
              "         [9.92532551e-01, 6.92349765e-03, 5.44037437e-04],\n",
              "         [9.94569838e-01, 4.95819980e-03, 4.71973879e-04],\n",
              "         [9.98663902e-01, 1.20707951e-03, 1.29073000e-04],\n",
              "         [9.97741580e-01, 2.10796879e-03, 1.50471955e-04],\n",
              "         [9.95123923e-01, 4.60237823e-03, 2.73693877e-04],\n",
              "         [9.98261988e-01, 1.58725481e-03, 1.50823776e-04],\n",
              "         [9.98117447e-01, 1.66510267e-03, 2.17488472e-04],\n",
              "         [9.98956561e-01, 9.41986626e-04, 1.01515136e-04],\n",
              "         [9.94731545e-01, 4.65001678e-03, 6.18365128e-04],\n",
              "         [8.65644813e-01, 1.27655372e-01, 6.69975672e-03],\n",
              "         [9.98962879e-01, 9.61940445e-04, 7.51710177e-05],\n",
              "         [9.99106348e-01, 8.36298510e-04, 5.72500976e-05],\n",
              "         [9.98937190e-01, 9.87627311e-04, 7.51964762e-05],\n",
              "         [9.96844649e-01, 2.94215581e-03, 2.13187697e-04],\n",
              "         [9.94291425e-01, 5.16543631e-03, 5.43111411e-04]]], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ1phPKisJMz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}