{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ecrGsYFSHOh"
      },
      "source": [
        "apostrophe_dict = {\n",
        "\"ain't\": \"am not / are not\",\n",
        "\"aren't\": \"are not / am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had / he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall / he will\",\n",
        "\"he'll've\": \"he shall have / he will have\",\n",
        "\"he's\": \"he has / he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has / how is\",\n",
        "\"i'd\": \"I had / I would\",\n",
        "\"i'd've\": \"I would have\",\n",
        "\"i'll\": \"I shall / I will\",\n",
        "\"i'll've\": \"I shall have / I will have\",\n",
        "\"i'm\": \"I am\",\n",
        "\"i've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had / it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall / it will\",\n",
        "\"it'll've\": \"it shall have / it will have\",\n",
        "\"it's\": \"it has / it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had / she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she shall have / she will have\",\n",
        "\"she's\": \"she has / she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as / so is\",\n",
        "\"that'd\": \"that would / that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has / that is\",\n",
        "\"there'd\": \"there had / there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there has / there is\",\n",
        "\"they'd\": \"they had / they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they shall / they will\",\n",
        "\"they'll've\": \"they shall have / they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had / we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what shall / what will\",\n",
        "\"what'll've\": \"what shall have / what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what has / what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has / when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where has / where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who shall / who will\",\n",
        "\"who'll've\": \"who shall have / who will have\",\n",
        "\"who's\": \"who has / who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why has / why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had / you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you shall / you will\",\n",
        "\"you'll've\": \"you shall have / you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "short_word_dict = {\n",
        "\"121\": \"one to one\",\n",
        "\"a/s/l\": \"age, sex, location\",\n",
        "\"adn\": \"any day now\",\n",
        "\"afaik\": \"as far as I know\",\n",
        "\"afk\": \"away from keyboard\",\n",
        "\"aight\": \"alright\",\n",
        "\"alol\": \"actually laughing out loud\",\n",
        "\"b4\": \"before\",\n",
        "\"b4n\": \"bye for now\",\n",
        "\"bak\": \"back at the keyboard\",\n",
        "\"bf\": \"boyfriend\",\n",
        "\"bff\": \"best friends forever\",\n",
        "\"bfn\": \"bye for now\",\n",
        "\"bg\": \"big grin\",\n",
        "\"bta\": \"but then again\",\n",
        "\"btw\": \"by the way\",\n",
        "\"cid\": \"crying in disgrace\",\n",
        "\"cnp\": \"continued in my next post\",\n",
        "\"cp\": \"chat post\",\n",
        "\"cu\": \"see you\",\n",
        "\"cul\": \"see you later\",\n",
        "\"cul8r\": \"see you later\",\n",
        "\"cya\": \"bye\",\n",
        "\"cyo\": \"see you online\",\n",
        "\"dbau\": \"doing business as usual\",\n",
        "\"fud\": \"fear, uncertainty, and doubt\",\n",
        "\"fwiw\": \"for what it's worth\",\n",
        "\"fyi\": \"for your information\",\n",
        "\"g\": \"grin\",\n",
        "\"g2g\": \"got to go\",\n",
        "\"ga\": \"go ahead\",\n",
        "\"gal\": \"get a life\",\n",
        "\"gf\": \"girlfriend\",\n",
        "\"gfn\": \"gone for now\",\n",
        "\"gmbo\": \"giggling my butt off\",\n",
        "\"gmta\": \"great minds think alike\",\n",
        "\"h8\": \"hate\",\n",
        "\"hagn\": \"have a good night\",\n",
        "\"hdop\": \"help delete online predators\",\n",
        "\"hhis\": \"hanging head in shame\",\n",
        "\"iac\": \"in any case\",\n",
        "\"ianal\": \"I am not a lawyer\",\n",
        "\"ic\": \"I see\",\n",
        "\"idk\": \"I don't know\",\n",
        "\"imao\": \"in my arrogant opinion\",\n",
        "\"imnsho\": \"in my not so humble opinion\",\n",
        "\"imo\": \"in my opinion\",\n",
        "\"iow\": \"in other words\",\n",
        "\"ipn\": \"I’m posting naked\",\n",
        "\"irl\": \"in real life\",\n",
        "\"jk\": \"just kidding\",\n",
        "\"l8r\": \"later\",\n",
        "\"ld\": \"later, dude\",\n",
        "\"ldr\": \"long distance relationship\",\n",
        "\"llta\": \"lots and lots of thunderous applause\",\n",
        "\"lmao\": \"laugh my ass off\",\n",
        "\"lmirl\": \"let's meet in real life\",\n",
        "\"lol\": \"laugh out loud\",\n",
        "\"ltr\": \"longterm relationship\",\n",
        "\"lulab\": \"love you like a brother\",\n",
        "\"lulas\": \"love you like a sister\",\n",
        "\"luv\": \"love\",\n",
        "\"m/f\": \"male or female\",\n",
        "\"m8\": \"mate\",\n",
        "\"milf\": \"mother I would like to fuck\",\n",
        "\"oll\": \"online love\",\n",
        "\"omg\": \"oh my god\",\n",
        "\"otoh\": \"on the other hand\",\n",
        "\"pir\": \"parent in room\",\n",
        "\"ppl\": \"people\",\n",
        "\"r\": \"are\",\n",
        "\"rofl\": \"roll on the floor laughing\",\n",
        "\"rpg\": \"role playing games\",\n",
        "\"ru\": \"are you\",\n",
        "\"shid\": \"slaps head in disgust\",\n",
        "\"somy\": \"sick of me yet\",\n",
        "\"sot\": \"short of time\",\n",
        "\"thanx\": \"thanks\",\n",
        "\"thx\": \"thanks\",\n",
        "\"ttyl\": \"talk to you later\",\n",
        "\"u\": \"you\",\n",
        "\"ur\": \"you are\",\n",
        "\"uw\": \"you’re welcome\",\n",
        "\"wb\": \"welcome back\",\n",
        "\"wfm\": \"works for me\",\n",
        "\"wibni\": \"wouldn't it be nice if\",\n",
        "\"wtf\": \"what the fuck\",\n",
        "\"wtg\": \"way to go\",\n",
        "\"wtgp\": \"want to go private\",\n",
        "\"ym\": \"young man\",\n",
        "\"gr8\": \"great\"\n",
        "}\n",
        "\n",
        "\n",
        "emoticon_dict = {\n",
        "\":)\": \"happy\",\n",
        "\":‑)\": \"happy\",\n",
        "\":-]\": \"happy\",\n",
        "\":-3\": \"happy\",\n",
        "\":->\": \"happy\",\n",
        "\"8-)\": \"happy\",\n",
        "\":-}\": \"happy\",\n",
        "\":o)\": \"happy\",\n",
        "\":c)\": \"happy\",\n",
        "\":^)\": \"happy\",\n",
        "\"=]\": \"happy\",\n",
        "\"=)\": \"happy\",\n",
        "\"<3\": \"happy\",\n",
        "\":-(\": \"sad\",\n",
        "\":(\": \"sad\",\n",
        "\":c\": \"sad\",\n",
        "\":<\": \"sad\",\n",
        "\":[\": \"sad\",\n",
        "\">:[\": \"sad\",\n",
        "\":{\": \"sad\",\n",
        "\">:(\": \"sad\",\n",
        "\":-c\": \"sad\",\n",
        "\":-< \": \"sad\",\n",
        "\":-[\": \"sad\",\n",
        "\":-||\": \"sad\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QHQOC6X3_-b"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kKAaQWm_pp8"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRWU-rLS2T5p",
        "outputId": "00c809be-1499-4575-e3fb-5db6ce52959c"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import tokenize as tknz\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoNFR3fO2cUn"
      },
      "source": [
        "#pd.set_option('display.max_columns', None)  \n",
        "#pd.set_option('display.expand_frame_repr', False)\n",
        "pd.set_option('max_colwidth', 800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an4_K-ZnHpLu"
      },
      "source": [
        "class TextPreprocessor:\n",
        "    def __init__(self, file, main_inf, csv_path = True):\n",
        "        '''\n",
        "        file - file path if csv_path true else pandas file\n",
        "        main_inf - base column name\n",
        "        '''\n",
        "        self.start_name = main_inf\n",
        "        self.last_col = None\n",
        "        if csv_path:\n",
        "            self.data = pd.read_csv(file)\n",
        "        else:\n",
        "            self.data = file.copy()\n",
        "        self.data_work = self.data.copy()\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.data.copy()\n",
        "\n",
        "    def get_data_work(self):\n",
        "        return self.data_work.copy()\n",
        "\n",
        "    def get_start_name(self):\n",
        "        return self.start_name\n",
        "\n",
        "    def clean_users(self):\n",
        "        regex = re.compile(\"@[\\w]*\")\n",
        "\n",
        "        def without_mention(text, regex=regex):\n",
        "            return re.sub(regex, ' ', text)\n",
        "        \n",
        "        work_col = self.start_name+'_wo_mention'\n",
        "\n",
        "        self.data_work[work_col] = None\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            self.data_work[work_col][i] = without_mention(self.data_work[self.start_name][i]).lower()\n",
        "\n",
        "        self.last_col = work_col\n",
        "\n",
        "    def change_words_by_dict(self, dict_to_check, dict_name):\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_wo_dist_'+dict_name\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_wo_dist_'+dict_name\n",
        "            where_col = self.start_name\n",
        "        \n",
        "        self.data_work[work_col] = None\n",
        "\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            text = self.data_work[where_col][i] \n",
        "            for word in text.split():\n",
        "                if word in dict_to_check:\n",
        "                    pos = text.find(word)\n",
        "                    text = text[:pos] + dict_to_check[word] +' ' + text[pos+len(word)+1:]\n",
        "            self.data_work[work_col][i] = text\n",
        "        \n",
        "        self.last_col = work_col\n",
        "    \n",
        "    def delete_punct(self): \n",
        "        regex2 = re.compile( r'[^\\w\\s]')\n",
        "\n",
        "        def wo_punct(text, regex=regex2):\n",
        "            return re.sub(regex, ' ', text)\n",
        "\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_wo_dist_punct'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_wo_dist_punct'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        self.data_work[work_col] = None\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            self.data_work[work_col][i] = wo_punct(self.data_work[where_col][i]).lower()\n",
        "\n",
        "        self.last_col = work_col\n",
        "\n",
        "\n",
        "    def delete_spec(self):\n",
        "        regex3 = re.compile( r'[^a-zA-Z0-9]')\n",
        "\n",
        "        def wo_spec(text, regex=regex3):\n",
        "            return re.sub(regex, ' ', text)\n",
        "\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_wo_dist_spec'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_wo_dist_spec'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        self.data_work[work_col] = None\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            self.data_work[work_col][i] = wo_spec(self.data_work[where_col][i]).lower()\n",
        "\n",
        "        self.last_col = work_col       \n",
        "        \n",
        "\n",
        "    def delete_numb(self): \n",
        "        regex4 = re.compile( r'[^a-zA-Z]')\n",
        "\n",
        "        def wo_numb(text, regex=regex4):\n",
        "            return re.sub(regex, ' ', text)\n",
        "\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_wo_dist_numb'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_wo_dist_numb'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        self.data_work[work_col] = None\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            self.data_work[work_col][i] = wo_numb(self.data_work[where_col][i]).lower()\n",
        "\n",
        "        self.last_col = work_col\n",
        "\n",
        "    def delete_short_words(self, length=1):\n",
        "\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_wo_short_words'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_wo_short_words'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        self.data_work[work_col] = None\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            text = self.data_work[where_col][i] \n",
        "            self.data_work[work_col][i] = ' '.join([w for w in text.split() if len(w)>length])\n",
        "        \n",
        "        self.last_col = work_col\n",
        "\n",
        "    def do_tokinze(self):\n",
        "\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_tokens'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_tokens'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        self.data_work[work_col] = None\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            text = self.data_work[where_col][i] \n",
        "            self.data_work[work_col][i] = tknz.word_tokenize(text)\n",
        "\n",
        "        self.last_col = work_col\n",
        "\n",
        "    \n",
        "    def wo_stopwords(self):\n",
        "\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_tokens_wo_stopwords'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_tokens_wo_stopwords'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "        self.data_work[work_col] = None\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            li = self.data_work[where_col][i] \n",
        "            self.data_work[work_col][i] = [word for word in li if not word in stop_words]\n",
        "\n",
        "        self.last_col = work_col\n",
        "\n",
        "\n",
        "    def stemmer(self):\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_stemm'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_stemm'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        stemmer = PorterStemmer()\n",
        "        self.data_work[work_col] = None\n",
        "        \n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            li = self.data_work[where_col][i]\n",
        "            res = [] \n",
        "            for word in li:\n",
        "                res.append(stemmer.stem(word))\n",
        "            self.data_work[work_col][i] = res\n",
        "      \n",
        "        self.last_col = work_col\n",
        "\n",
        "    def lemmer(self):\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_lemm'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_lemm'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        self.data_work[work_col] = None\n",
        "\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            li = self.data_work[where_col][i]\n",
        "            res = [] \n",
        "            for word in li:\n",
        "                res.append(lemmatizer.lemmatize(word))\n",
        "            self.data_work[work_col][i] = ' '.join(res)\n",
        "        \n",
        "        self.last_col = work_col\n",
        "    \n",
        "    def add_final_inf_to_data(self):\n",
        "        self.data['Preproc_text'] = self.data_work[self.last_col]\n",
        "\n",
        "    def full_preprocessor(self):\n",
        "        self.clean_users()\n",
        "        self.change_words_by_dict(apostrophe_dict, 'apostr')\n",
        "        self.change_words_by_dict(short_word_dict, 'shw')\n",
        "        self.change_words_by_dict(emoticon_dict, 'emot')\n",
        "        self.delete_punct()\n",
        "        self.delete_spec()\n",
        "        self.delete_numb()\n",
        "        self.delete_short_words()\n",
        "        self.do_tokinze()\n",
        "        self.wo_stopwords()\n",
        "        self.stemmer()\n",
        "        self.lemmer()\n",
        "        self.add_final_inf_to_data()\n",
        "      \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cNXPnnWuNON0",
        "outputId": "ae05771e-721e-42fb-b838-726c2f534e79"
      },
      "source": [
        "TP = TextPreprocessor('train_tweets.csv', 'tweet')\n",
        "TP.full_preprocessor()\n",
        "TP.get_data_work()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:93: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:113: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:129: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:145: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:164: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:185: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:205: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_wo_mention</th>\n",
              "      <th>tweet_wo_dist_apostr</th>\n",
              "      <th>tweet_wo_dist_shw</th>\n",
              "      <th>tweet_wo_dist_emot</th>\n",
              "      <th>tweet_wo_dist_punct</th>\n",
              "      <th>tweet_wo_dist_spec</th>\n",
              "      <th>tweet_wo_dist_numb</th>\n",
              "      <th>tweet_wo_short_words</th>\n",
              "      <th>tweet_tokens</th>\n",
              "      <th>tweet_tokens_wo_stopwords</th>\n",
              "      <th>tweet_stemm</th>\n",
              "      <th>tweet_lemm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
              "      <td>when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
              "      <td>when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
              "      <td>when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
              "      <td>when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
              "      <td>when a father is dysfunctional and is so selfish he drags his kids into his dysfunction     run</td>\n",
              "      <td>when a father is dysfunctional and is so selfish he drags his kids into his dysfunction     run</td>\n",
              "      <td>when a father is dysfunctional and is so selfish he drags his kids into his dysfunction     run</td>\n",
              "      <td>when father is dysfunctional and is so selfish he drags his kids into his dysfunction run</td>\n",
              "      <td>[when, father, is, dysfunctional, and, is, so, selfish, he, drags, his, kids, into, his, dysfunction, run]</td>\n",
              "      <td>[father, dysfunctional, selfish, drags, kids, dysfunction, run]</td>\n",
              "      <td>[father, dysfunct, selfish, drag, kid, dysfunct, run]</td>\n",
              "      <td>father dysfunct selfish drag kid dysfunct run</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
              "      <td>thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
              "      <td>thanks for #lyft credit i cannot use cause they do not offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
              "      <td>thanks for #lyft credit i cannot use cause they do not offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
              "      <td>thanks for #lyft credit i cannot use cause they do not offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
              "      <td>thanks for  lyft credit i cannot use cause they do not offer wheelchair vans in pdx      disapointed  getthanked</td>\n",
              "      <td>thanks for  lyft credit i cannot use cause they do not offer wheelchair vans in pdx      disapointed  getthanked</td>\n",
              "      <td>thanks for  lyft credit i cannot use cause they do not offer wheelchair vans in pdx      disapointed  getthanked</td>\n",
              "      <td>thanks for lyft credit cannot use cause they do not offer wheelchair vans in pdx disapointed getthanked</td>\n",
              "      <td>[thanks, for, lyft, credit, can, not, use, cause, they, do, not, offer, wheelchair, vans, in, pdx, disapointed, getthanked]</td>\n",
              "      <td>[thanks, lyft, credit, use, cause, offer, wheelchair, vans, pdx, disapointed, getthanked]</td>\n",
              "      <td>[thank, lyft, credit, use, caus, offer, wheelchair, van, pdx, disapoint, getthank]</td>\n",
              "      <td>thank lyft credit use caus offer wheelchair van pdx disapoint getthank</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>[bihday, your, majesty]</td>\n",
              "      <td>[bihday, majesty]</td>\n",
              "      <td>[bihday, majesti]</td>\n",
              "      <td>bihday majesti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦</td>\n",
              "      <td>#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦</td>\n",
              "      <td>#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦</td>\n",
              "      <td>#model   i love yoyou take with u all the time in urð±!!! ððððð¦ð¦ð¦</td>\n",
              "      <td>#model   i love yoyou take with u all the time in urð±!!! ððððð¦ð¦ð¦</td>\n",
              "      <td>model   i love yoyou take with u all the time in urð       ð   ð   ð   ð  ð   ð   ð</td>\n",
              "      <td>model   i love yoyou take with u all the time in ur</td>\n",
              "      <td>model   i love yoyou take with u all the time in ur</td>\n",
              "      <td>model love yoyou take with all the time in ur</td>\n",
              "      <td>[model, love, yoyou, take, with, all, the, time, in, ur]</td>\n",
              "      <td>[model, love, yoyou, take, time, ur]</td>\n",
              "      <td>[model, love, yoyou, take, time, ur]</td>\n",
              "      <td>model love yoyou take time ur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>factsguide  society now     motivation</td>\n",
              "      <td>factsguide  society now     motivation</td>\n",
              "      <td>factsguide  society now     motivation</td>\n",
              "      <td>factsguide society now motivation</td>\n",
              "      <td>[factsguide, society, now, motivation]</td>\n",
              "      <td>[factsguide, society, motivation]</td>\n",
              "      <td>[factsguid, societi, motiv]</td>\n",
              "      <td>factsguid societi motiv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31957</th>\n",
              "      <td>31958</td>\n",
              "      <td>0</td>\n",
              "      <td>ate @user isz that youuu?ðððððððððâ¤ï¸</td>\n",
              "      <td>ate   isz that youuu?ðððððððððâ¤ï¸</td>\n",
              "      <td>ate   isz that youuu?ðððððððððâ¤ï¸</td>\n",
              "      <td>ate   isz that youuu?ðððððððððâ¤ï¸</td>\n",
              "      <td>ate   isz that youuu?ðððððððððâ¤ï¸</td>\n",
              "      <td>ate   isz that youuu ð   ð   ð   ð   ð   ð   ð   ð   ð   â  ï</td>\n",
              "      <td>ate   isz that youuu</td>\n",
              "      <td>ate   isz that youuu</td>\n",
              "      <td>ate isz that youuu</td>\n",
              "      <td>[ate, isz, that, youuu]</td>\n",
              "      <td>[ate, isz, youuu]</td>\n",
              "      <td>[ate, isz, youuu]</td>\n",
              "      <td>ate isz youuu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31958</th>\n",
              "      <td>31959</td>\n",
              "      <td>0</td>\n",
              "      <td>to see nina turner on the airwaves trying to wrap herself in the mantle of a genuine hero like shirley chisolm. #shame #imwithher</td>\n",
              "      <td>to see nina turner on the airwaves trying to wrap herself in the mantle of a genuine hero like shirley chisolm. #shame #imwithher</td>\n",
              "      <td>to see nina turner on the airwaves trying to wrap herself in the mantle of a genuine hero like shirley chisolm. #shame #imwithher</td>\n",
              "      <td>to see nina turner on the airwaves trying to wrap herself in the mantle of a genuine hero like shirley chisolm. #shame #imwithher</td>\n",
              "      <td>to see nina turner on the airwaves trying to wrap herself in the mantle of a genuine hero like shirley chisolm. #shame #imwithher</td>\n",
              "      <td>to see nina turner on the airwaves trying to wrap herself in the mantle of a genuine hero like shirley chisolm   shame  imwithher</td>\n",
              "      <td>to see nina turner on the airwaves trying to wrap herself in the mantle of a genuine hero like shirley chisolm   shame  imwithher</td>\n",
              "      <td>to see nina turner on the airwaves trying to wrap herself in the mantle of a genuine hero like shirley chisolm   shame  imwithher</td>\n",
              "      <td>to see nina turner on the airwaves trying to wrap herself in the mantle of genuine hero like shirley chisolm shame imwithher</td>\n",
              "      <td>[to, see, nina, turner, on, the, airwaves, trying, to, wrap, herself, in, the, mantle, of, genuine, hero, like, shirley, chisolm, shame, imwithher]</td>\n",
              "      <td>[see, nina, turner, airwaves, trying, wrap, mantle, genuine, hero, like, shirley, chisolm, shame, imwithher]</td>\n",
              "      <td>[see, nina, turner, airwav, tri, wrap, mantl, genuin, hero, like, shirley, chisolm, shame, imwithh]</td>\n",
              "      <td>see nina turner airwav tri wrap mantl genuin hero like shirley chisolm shame imwithh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31959</th>\n",
              "      <td>31960</td>\n",
              "      <td>0</td>\n",
              "      <td>listening to sad songs on a monday morning otw to work is sad</td>\n",
              "      <td>listening to sad songs on a monday morning otw to work is sad</td>\n",
              "      <td>listening to sad songs on a monday morning otw to work is sad</td>\n",
              "      <td>listening to sad songs on a monday morning otw to work is sad</td>\n",
              "      <td>listening to sad songs on a monday morning otw to work is sad</td>\n",
              "      <td>listening to sad songs on a monday morning otw to work is sad</td>\n",
              "      <td>listening to sad songs on a monday morning otw to work is sad</td>\n",
              "      <td>listening to sad songs on a monday morning otw to work is sad</td>\n",
              "      <td>listening to sad songs on monday morning otw to work is sad</td>\n",
              "      <td>[listening, to, sad, songs, on, monday, morning, otw, to, work, is, sad]</td>\n",
              "      <td>[listening, sad, songs, monday, morning, otw, work, sad]</td>\n",
              "      <td>[listen, sad, song, monday, morn, otw, work, sad]</td>\n",
              "      <td>listen sad song monday morn otw work sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31960</th>\n",
              "      <td>31961</td>\n",
              "      <td>1</td>\n",
              "      <td>@user #sikh #temple vandalised in in #calgary, #wso condemns  act</td>\n",
              "      <td>#sikh #temple vandalised in in #calgary, #wso condemns  act</td>\n",
              "      <td>#sikh #temple vandalised in in #calgary, #wso condemns  act</td>\n",
              "      <td>#sikh #temple vandalised in in #calgary, #wso condemns  act</td>\n",
              "      <td>#sikh #temple vandalised in in #calgary, #wso condemns  act</td>\n",
              "      <td>sikh  temple vandalised in in  calgary   wso condemns  act</td>\n",
              "      <td>sikh  temple vandalised in in  calgary   wso condemns  act</td>\n",
              "      <td>sikh  temple vandalised in in  calgary   wso condemns  act</td>\n",
              "      <td>sikh temple vandalised in in calgary wso condemns act</td>\n",
              "      <td>[sikh, temple, vandalised, in, in, calgary, wso, condemns, act]</td>\n",
              "      <td>[sikh, temple, vandalised, calgary, wso, condemns, act]</td>\n",
              "      <td>[sikh, templ, vandalis, calgari, wso, condemn, act]</td>\n",
              "      <td>sikh templ vandalis calgari wso condemn act</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31961</th>\n",
              "      <td>31962</td>\n",
              "      <td>0</td>\n",
              "      <td>thank you @user for you follow</td>\n",
              "      <td>thank you   for you follow</td>\n",
              "      <td>thank you   for you follow</td>\n",
              "      <td>thank you   for you follow</td>\n",
              "      <td>thank you   for you follow</td>\n",
              "      <td>thank you   for you follow</td>\n",
              "      <td>thank you   for you follow</td>\n",
              "      <td>thank you   for you follow</td>\n",
              "      <td>thank you for you follow</td>\n",
              "      <td>[thank, you, for, you, follow]</td>\n",
              "      <td>[thank, follow]</td>\n",
              "      <td>[thank, follow]</td>\n",
              "      <td>thank follow</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>31962 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                                                            tweet_lemm\n",
              "0          1  ...                                         father dysfunct selfish drag kid dysfunct run\n",
              "1          2  ...                thank lyft credit use caus offer wheelchair van pdx disapoint getthank\n",
              "2          3  ...                                                                        bihday majesti\n",
              "3          4  ...                                                         model love yoyou take time ur\n",
              "4          5  ...                                                               factsguid societi motiv\n",
              "...      ...  ...                                                                                   ...\n",
              "31957  31958  ...                                                                         ate isz youuu\n",
              "31958  31959  ...  see nina turner airwav tri wrap mantl genuin hero like shirley chisolm shame imwithh\n",
              "31959  31960  ...                                              listen sad song monday morn otw work sad\n",
              "31960  31961  ...                                           sikh templ vandalis calgari wso condemn act\n",
              "31961  31962  ...                                                                          thank follow\n",
              "\n",
              "[31962 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baVCoZ997BJs"
      },
      "source": [
        "TP.get_data().to_pickle(path='tweet_prepared.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}